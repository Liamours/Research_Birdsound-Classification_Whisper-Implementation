{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"EVu2yCtcwvB2","executionInfo":{"status":"ok","timestamp":1743485228577,"user_tz":-420,"elapsed":7,"user":{"displayName":"Lulay Liam","userId":"12138696513278522129"}}},"outputs":[],"source":["import os\n","import torch\n","import torchaudio\n","import torchaudio.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","import torch.nn as nn\n","from tqdm import tqdm\n","import numpy as np\n","\n","import gdown\n","import zipfile\n","from google.colab import drive"]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")\n","\n","dir_main = \"/content/drive/MyDrive/Penelitian: Birdsound_Classification/dataset_12\"\n","dir_train = dir_main + \"/train_grouped\"\n","dir_val = dir_main + \"/val_grouped\"\n","dir_test = dir_main + \"/test_grouped\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kFOi4bMENsr","executionInfo":{"status":"ok","timestamp":1743485231604,"user_tz":-420,"elapsed":3023,"user":{"displayName":"Lulay Liam","userId":"12138696513278522129"}},"outputId":"265a3820-fa78-4f7d-fa87-f5eec09ccc21"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","N_MELS = 96\n","N_CTX = 512\n","N_STATE = 512\n","N_HEAD = 8\n","N_LAYER = 6\n","EPOCHS = 10\n","LEARNING_RATE = 0.001"],"metadata":{"id":"MluOzuZPy1_G","executionInfo":{"status":"ok","timestamp":1743485231622,"user_tz":-420,"elapsed":33,"user":{"displayName":"Lulay Liam","userId":"12138696513278522129"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class BirdAudioDataset(Dataset):\n","    def __init__(self, root_dir, target_time=512, target_mels=96, mean=None, std=None):\n","        self.target_time = target_time\n","        self.target_mels = target_mels\n","        self.file_paths = []\n","        self.labels = []\n","        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","        first_file = os.path.join(root_dir, self.classes[0], os.listdir(os.path.join(root_dir, self.classes[0]))[0])\n","        self.sample_rate = torchaudio.info(first_file).sample_rate\n","\n","        self.n_mels = target_mels\n","        self.n_fft = 2048\n","        self.hop_length = 278\n","        self.mean = mean\n","        self.std = std\n","\n","        for class_name in self.classes:\n","            class_dir = os.path.join(root_dir, class_name)\n","            for file in os.listdir(class_dir):\n","                if file.endswith(('.wav', '.mp3')):\n","                    self.file_paths.append(os.path.join(class_dir, file))\n","                    self.labels.append(self.class_to_idx[class_name])\n","\n","    def compute_log_mel_spectrogram(self, waveform):\n","        mel_spectrogram = T.MelSpectrogram(\n","            sample_rate=self.sample_rate,\n","            n_fft=self.n_fft,\n","            hop_length=self.hop_length,\n","            n_mels=self.n_mels,\n","            window_fn=torch.hann_window\n","        )(waveform)\n","\n","        mel_spectrogram = torch.log(mel_spectrogram + 1e-6)\n","\n","        if self.mean is not None and self.std is not None:\n","            mel_spectrogram = (mel_spectrogram - self.mean) / (self.std + 1e-6)\n","        else:\n","            mel_spectrogram = (mel_spectrogram - mel_spectrogram.mean()) / (mel_spectrogram.std() + 1e-6)\n","\n","        if mel_spectrogram.shape[-1] < self.target_time:\n","            pad = self.target_time - mel_spectrogram.shape[-1]\n","            mel_spectrogram = F.pad(mel_spectrogram, (0, pad))\n","        else:\n","            mel_spectrogram = mel_spectrogram[..., :self.target_time]\n","\n","        if mel_spectrogram.shape[-2] < self.target_mels:\n","            pad = self.target_mels - mel_spectrogram.shape[-2]\n","            mel_spectrogram = F.pad(mel_spectrogram, (0, 0, 0, pad))\n","        else:\n","            mel_spectrogram = mel_spectrogram[..., :self.target_mels, :]\n","\n","        return mel_spectrogram\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            waveform, sr = torchaudio.load(self.file_paths[idx])\n","            if sr != self.sample_rate:\n","                waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n","            if waveform.shape[0] > 1:\n","                waveform = torch.mean(waveform, dim=0, keepdim=True)\n","            log_mel_spec = self.compute_log_mel_spectrogram(waveform)\n","            label = torch.tensor(self.labels[idx], dtype=torch.long)\n","            return log_mel_spec.unsqueeze(-1), label\n","        except Exception as e:\n","            print(f\"Error loading {self.file_paths[idx]}: {e}\")\n","            return self[np.random.randint(0, len(self)-1)]\n","\n","def compute_dataset_statistics(dataset, num_samples=None):\n","    if num_samples is None:\n","        num_samples = len(dataset)\n","\n","    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n","\n","    specs = []\n","    for i in indices:\n","        spec, _ = dataset[i]\n","        specs.append(spec)\n","\n","    specs = torch.stack(specs)\n","    mean = specs.mean()\n","    std = specs.std()\n","\n","    return mean, std\n","\n","temp_train_dataset = BirdAudioDataset(dir_train)\n","temp_val_dataset = BirdAudioDataset(dir_val)\n","temp_test_dataset = BirdAudioDataset(dir_test)\n","\n","train_mean, train_std = compute_dataset_statistics(temp_train_dataset)\n","\n","train_dataset = BirdAudioDataset(dir_train, mean=train_mean, std=train_std)\n","val_dataset = BirdAudioDataset(dir_val, mean=train_mean, std=train_std)\n","test_dataset = BirdAudioDataset(dir_test, mean=train_mean, std=train_std)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n","\n","N_CLASSES = len(train_dataset.classes)"],"metadata":{"id":"nUvPJy9XcY0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NkWOJKZbwFd6"},"outputs":[],"source":["class AudioEncoder(nn.Module):\n","    def __init__(self, n_mels, n_ctx, n_state, n_head, n_layer, n_classes):\n","        super(AudioEncoder, self).__init__()\n","        self.conv1 = nn.Conv2d(1, n_state, kernel_size=(3,3), padding=1)\n","        self.conv2 = nn.Conv2d(n_state, n_state, kernel_size=(3,3), stride=(2,2), padding=1)\n","        self.conv3 = nn.Conv2d(n_state, n_state, kernel_size=(3,3), stride=(2,2), padding=1)\n","        self.conv4 = nn.Conv2d(n_state, n_state, kernel_size=(3,3), stride=(2,2), padding=1)\n","        self.register_buffer(\"positional_embedding\", torch.randn(1, n_ctx, n_state))\n","        self.blocks = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=n_state, nhead=n_head, activation=\"gelu\")\n","            for _ in range(n_layer)\n","        ])\n","        self.ln_post = nn.LayerNorm(n_state)\n","        self.fc1 = nn.Linear(n_state, n_state // 2)\n","        self.fc2 = nn.Linear(n_state // 2, n_classes)\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = x.permute(0, 3, 1, 2)\n","        x = self.dropout(F.gelu(self.conv1(x)))\n","        x = self.dropout(F.gelu(self.conv2(x)))\n","        x = self.dropout(F.gelu(self.conv3(x)))\n","        x = self.dropout(F.gelu(self.conv4(x)))\n","        seq_len = x.shape[2] * x.shape[3]\n","        x = x.reshape(x.size(0), seq_len, x.size(1))\n","        pos_emb = self.positional_embedding[:, :x.shape[1], :]\n","        x = (x + pos_emb).to(x.dtype)\n","        for block in self.blocks:\n","            x = block(x)\n","        x = self.ln_post(x)\n","        x = self.dropout(F.gelu(self.fc1(x)))\n","        x = self.fc2(F.gelu(x[:, -1, :]))\n","        return x\n","\n","model = AudioEncoder(n_mels=N_MELS, n_ctx=N_CTX, n_state=N_STATE, n_head=N_HEAD, n_layer=N_LAYER, n_classes=N_CLASSES)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","criterion = nn.CrossEntropyLoss()\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuPk0mISw1Ri"},"outputs":[],"source":["def train_one_epoch(epoch):\n","    model.train()\n","    total_loss, total_correct = 0, 0\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","    for batch, labels in progress_bar:\n","        batch, labels = batch.to(device), labels.to(device)\n","        output = model(batch)\n","        loss = criterion(output, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_correct += (output.argmax(1) == labels).sum().item()\n","        progress_bar.set_postfix(loss=total_loss/len(train_loader), acc=total_correct/(batch_idx * batch_size + len(batch)))\n","\n","def evaluate(loader, dataset_name):\n","    model.eval()\n","    total_correct, total_samples = 0, 0\n","    with torch.no_grad():\n","        for batch, labels in tqdm(loader, desc=f\"Evaluating {dataset_name}\"):\n","            batch, labels = batch.to(device), labels.to(device)\n","            output = model(batch)\n","            total_correct += (output.argmax(1) == labels).sum().item()\n","            total_samples += labels.size(0)\n","    accuracy = total_correct / total_samples\n","    print(f\"{dataset_name} Accuracy: {accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9g3aTeUFwX5I"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    train_one_epoch(epoch)\n","    evaluate(val_loader, \"Validation\")"]},{"cell_type":"code","source":["evaluate(test_loader, \"Test\")"],"metadata":{"id":"qNTJdlHMuorE"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}